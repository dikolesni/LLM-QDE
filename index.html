<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Simulating Expert Behavior with LLMs for Question Difficulty Estimation</title>
  <style>
    body {
      font-family: "Helvetica Neue", Arial, sans-serif;
      margin: 0;
      padding: 0;
      background: #fafafa;
      color: #222;
      line-height: 1.6;
    }
    header {
      text-align: center;
      padding: 2rem;
      background: #ffd54f;
    }
    h1 {
      font-size: 2.5rem;
      margin-bottom: 0.5rem;
    }
    h2 {
      margin-top: 2rem;
      margin-bottom: 1rem;
      color: #003366;
    }
    section {
      max-width: 1000px;
      margin: auto;
      padding: 2rem;
      background: white;
    }
    p {
      margin-bottom: 1.2rem;
    }
    ul, ol {
      margin: 1rem 0 1.2rem 2rem;
    }
    figure {
      text-align: center;
      margin: 2rem 0;
    }
    img {
      max-width: 100%;
      height: auto;
      border-radius: 8px;
    }
    footer {
      text-align: center;
      padding: 1rem;
      font-size: 0.9rem;
      color: #666;
    }
    a {
      color: #003366;
      text-decoration: underline;
    }
    a:hover {
      color: #ff6f00;
    }
  </style>
</head>
<body>
  <header>
    <h1>Simulating Expert Behavior with LLMs for Question Difficulty Estimation: Angoff-like Versus Pairwise Procedures</h1>
    <p><strong>Author(s):</strong> Diana Kolesnikova · <strong>Affiliation:</strong> Tilburg University</p>
    <p><strong>Author(s):</strong> Kirill Fedyanin · <strong>Affiliation:</strong> srb.tech</p>
    <p><strong>Author(s):</strong> Dr. Matthieu Brinkhuis · <strong>Affiliation:</strong> Utrecht University</p>
    <p><strong>Author(s):</strong> Dr. Maria Bolsinova · <strong>Affiliation:</strong> Tilburg University</p>
  </header>

  <section>
    <h2>Problem</h2>
    <p>Reliable estimates of task difficulty are essential for building valid assessments (Wainer & Mislevy, 2000) and efficient learning systems (Wauters, Desmet, & Van Den Noortgate, 2010). Accurate knowledge of task difficulty enables educators and assessment developers to mitigate the cold-start problem inherent to adaptive systems (van der Velde et al., 2021; Klinkenberg et al., 2011), construct parallel test forms (Van der Linden & Adema, 1998), and design learning trajectories that minimize time-to-mastery while maximizing learner engagement (Kulik & Fletcher, 2016; VanLehn, 2011).</p>

    <p>Difficulty is typically conceptualized as a latent characteristic of an item, influencing the probability of a correct response (e.g., Hambleton et al., 1991). Practitioners usually focus on difficulty estimates derived empirically from an actual population of respondents or learners. In these cases, difficulty is commonly quantified through Classical Test Theory (CTT) methods, such as proportions of correct answers (p-values; Crocker & Algina, 1986) or through Item Response Theory (IRT) models (Lord & Novick, 1968), for instance via difficulty parameters derived from the Rasch model (Rasch, 1960). These empirically derived values are frequently treated as the ground truth for subsequent analysis (AlKhuzaey et al., 2024; Benedetto et al., 2020).</p>

    <p>However, in practical contexts, response data for new items are often unavailable, requiring additional steps for difficulty estimation. Traditional strategies for estimating the difficulty of newly developed items before exposure to the target main audience frequently rely on pre-testing with a smaller sample of target respondents. While this approach provides direct evidence, it incurs substantial time and financial costs (Jalili, Hejri, & Norcini, 2011). More importantly, it creates additional risks of item exposure and content leaking, which is undesirable for high-stakes testing (Ozaki & Toyoda, 2006).</p>

    <h2>Idea</h2>
    <p>To address these limitations, the present study evaluates the capacity of off-the-shelf LLMs to simulate expert difficulty judgments without extensive datasets. Two standard expert-rating procedures—an Angoff-like method and pairwise comparison—are implemented under two experimental factors:</p>

    <ul>
      <li>zero-shot versus few-shot prompting;</li>
      <li>hard-decision versus soft-probability outputs.</li>
    </ul>

    <h2>Procedure</h2>
    <p>The evaluation encompasses three state-of-the-art LLMs, including both proprietary and open-source models, applied across the item bank of MathGarden (approximately 2 000 Dutch-language mathematics items for primary school). Six domains with sixty items in each are sampled. Predicted item difficulties are compared with empirically derived difficulty estimates (IRT approach) using Spearman rank correlation.</p>

    <h2>Detailed Results</h2>
    <p>Here you can find tables with detailed results:</p>
    <p><a href="https://docs.google.com/spreadsheets/d/1tovrij73_Q1rxdlKMdcbxJbEMoodTUmuZb3nNkOMnSE/edit?usp=sharing" target="_blank">Open Google Sheets</a></p>

    <h2>Challenges</h2>
    <p>The topic of difficulty estimation is vast. Other ideas that should be explored in future include two big directions – increasing reliability of estimation and improving interpretability of it. These two improvements could be potentially achieved through the following ideas:</p>

    <ul>
      <li>sending prompts within one context (memory);</li>
      <li>checking the work of reasoning models;</li>
      <li>using ranking procedures, in particular ranking triplets;</li>
      <li>exploring task-features that imply difficulty to LLMs;</li>
      <li>comparing LLMs thinking patterns and human experts’ cognitive processes.</li>
    </ul>
  </section>

  <footer>
    <p>© 2025 Kolesnikova Diana · dianakolesnikova@gmail.com · Presented at AEA Europe</p>
  </footer>
</body>
</html>
